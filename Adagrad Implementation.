import numpy as np 

def gradient(x):
    return 2 * x + 3

def adagrad_optimizer(initial_x, learning_rate=.2, epsilon=1e-8, num_iterations=5):
    x = initial_x
    accumulated_gradient = 0

    for i in range(num_iterations):
        grad = gradient(x)
        accumulated_gradient += grad ** 2
        x -= (learning_rate / (np.sqrt(accumulated_gradient) + epsilon)) * grad
        print(f"Iteration {i + 1}: x = {x:.4f}, gradient = {grad:.4f}")

    return x

optimized_x = adagrad_optimizer(initial_x = 10.0, learning_rate = 1, num_iterations = 5)
print(f"Optimized value of x: {optimized_x:.4f}")
