import numpy as np

def func(x):
    x1, x2 = x
    return (x1 - 1) ** 2 + 2 * (x2 - 2) ** 2

def grad(x):
    x1, x2 = x
    df_dx1 = 2 * (x1 - 1)
    df_dx2 = 4 * (x2 - 2)
    return np.array([df_dx1, df_dx2])

def conjugate_gradient_fletcher_reeves(func, grad, x0, tol=1e-5, max_iter=1000):
    x = x0
    r = -grad(x)
    p = r
    rs_old = np.dot(r.T, r)

    for i in range(max_iter):
        alpha = line_search(func, grad, x, p)
        x = x + alpha * p

        r_new = -grad(x)
        rs_new = np.dot(r_new.T, r_new)

        if np.sqrt(rs_new) < tol:
            print(f'Converged in {i + 1} iterations.')
            break

        beta = rs_new / rs_old
        p = r_new + beta * p 
        r = r_new
        rs_old = rs_new

    return x

def line_search(func, grad, x, p, alpha=1, beta=0.5, c=1e-4):
    """Backtracking line search to find step size satisfying Wolfe conditions"""
    while func(x + alpha * p) > func(x) + c * alpha * np.dot(grad(x).T, p):
        alpha *= beta
    return alpha

x0 = np.array([0, 0])

x_min = conjugate_gradient_fletcher_reeves(func, grad, x0)
print("Optimized Solution:", x_min)
